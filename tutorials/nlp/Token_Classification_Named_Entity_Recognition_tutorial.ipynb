{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Token_Classification_Named Entity Recognition_tutorial.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRLPr0TnIAHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BRANCH = 'ner_tutorial'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_0K1lsW1dj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell\n",
        "\n",
        "# install NeMo\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[nlp]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzqD2WDFOIN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nemo.utils.exp_manager import exp_manager\n",
        "from nemo.collections import nlp as nemo_nlp\n",
        "\n",
        "import os\n",
        "import wget \n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from omegaconf import OmegaConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daYw_Xll2ZR9",
        "colab_type": "text"
      },
      "source": [
        "# Task Description\n",
        "**Named entity recognition (NER)**, also referred to as entity chunking, identification or extraction, is the task of detecting and classifying key information (entities) in text.\n",
        "For example, in a sentence:  `Mary leaves in Santa Clara and works at NVIDIA.`, we should detect that `Mary` is a person, `Santa Clara` is location and `NVIDIA` is a company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnuziSwJ1yEB",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In this tutorial we going to use [GMB(Groningen Meaning Bank)](http://www.let.rug.nl/bjerva/gmb/about.php) corpus for entity recognition. \n",
        "\n",
        "GMB is a fairly large corpus with a lot of annotations. Note, that GMB is not completely human annotated and it’s not considered 100% correct. \n",
        "The data is labeled using the IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) (short for inside, outside, beginning). \n",
        "\n",
        "The following classes appear in the dataset:\n",
        "* LOC = Geographical Entity\n",
        "* ORG = Organization\n",
        "* PER = Person\n",
        "* GPE = Geopolitical Entity\n",
        "* TIME = Time indicator\n",
        "* ART = Artifact\n",
        "* EVE = Event\n",
        "* NAT = Natural Phenomenon\n",
        "\n",
        "For this tutorial, classes ART, EVE, and NAT were combined into a MISC class due to small number of examples for these classes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcZ3nb_-SVT",
        "colab_type": "text"
      },
      "source": [
        "# NeMo Token Classification Data Format\n",
        "\n",
        "[TokenClassification Model](https://github.com/NVIDIA/NeMo/blob/candidate/nemo/collections/nlp/models/token_classification/token_classification_model.py) in NeMo support NER and other token level classification tasks, as long as the data followes the format specified below. \n",
        "\n",
        "For NeMo Token Classification training scripts requires the data to be splitted into 2 files: \n",
        "* text.txt and \n",
        "* labels.txt. \n",
        "\n",
        "Each line of the **text.txt** file contains text sequences, where words are separated with spaces. The **labels.txt** file contains corresponding labels for each word in text.txt, the labels are separated with spaces. Each line of the files should follow the format: [WORD] [SPACE] [WORD] [SPACE] [WORD] (for text.txt) and [LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for labels.txt).\n",
        "\n",
        "Example of a text.txt file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXFORGBv2Jqu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Jennifer is from New York City .\n",
        "She likes ...\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWNBlHcT-N6q",
        "colab_type": "text"
      },
      "source": [
        "Corresponding labels.txt file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-94C1-864EW1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "B-PER O O B-LOC I-LOC I-LOC O\n",
        "O O ...\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsEmwIPO4L4V",
        "colab_type": "text"
      },
      "source": [
        "To convert an IOB format data to the format reqired for training, use this [script](https://github.com/NVIDIA/NeMo/blob/master/examples/nlp/token_classification/import_from_iob_format.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL58EWkd2ZVb",
        "colab_type": "text"
      },
      "source": [
        "## Download and preprocess the data¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrx2ZXHrCHb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhUzIeF0Yg0l",
        "colab_type": "text"
      },
      "source": [
        "Let's extract files from the .zip file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y01BdjPRW-7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = 'for_tutorial'\n",
        "data_zip = '/My\\ Drive/Colab\\ Notebooks/' + DATA_DIR + '.zip'\n",
        "! unzip drive$data_zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Ty5_S7Ye8h",
        "colab_type": "text"
      },
      "source": [
        "Now, the data folder should contain 4 files:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8vsyh3JZH26",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "* labels_dev.txt\n",
        "* labels_train.txt\n",
        "* text_dev.txt\n",
        "* text_train.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QTXmg7kYMaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls -l $DATA_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8HZrDmr12_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORK_DIR = \"PATH_TO_CHECKPOINTS_AND_LOGS\"\n",
        "MODEL_CONFIG = \"token_classification_config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UDPgadLN6SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's take a look at the data \n",
        "print('Text:')\n",
        "! head -n 5 {DATA_DIR}/text_train.txt\n",
        "\n",
        "print('\\nLabels:')\n",
        "! head -n 5 {DATA_DIR}/labels_train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daludzzL2Jba",
        "colab_type": "text"
      },
      "source": [
        "# Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_whKCxfTMo6Y",
        "colab_type": "text"
      },
      "source": [
        "Out Named Entity Recognition model is comprised of the pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a Token Classification layer.\n",
        "\n",
        "The model is defined in a config file which declares multiple important sections. They are:\n",
        "- **model**: All arguments that are related to the Model - language model, token classifiers, optimizer and schedulers, datasets and any other related information\n",
        "\n",
        "- **trainer**: Any argument to be passed to PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1gA8PsJ13MJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the model's configuration file \n",
        "config_dir = WORK_DIR + '/configs/'\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
        "    print('Downloading config file...')\n",
        "    wget.download('https://raw.githubusercontent.com/NVIDIA/NeMo/' + BRANCH + '/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
        "else:\n",
        "    print ('config file is already exists')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3KmWMvSUQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this line will print the entire config of the model\n",
        "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
        "print(config_path)\n",
        "config = OmegaConf.load(config_path)\n",
        "print(OmegaConf.to_yaml(config))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCgWzNBkaQLZ",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Data within the config\n",
        "\n",
        "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
        "\n",
        "We assume that both training and evaluation files are located in the same directory, and use the default names mentioned during the data download step. \n",
        "So, to start model training, simply specify `model.dataset.data_dir`, like we are going to do below.\n",
        "\n",
        "Also notice that some configs, including `model.dataset.data_dir`, have `???` in place of paths, this values are required to be specified by the user.\n",
        "\n",
        "Let's now add the data directory path to the config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQHCJN-ZaoLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in this tutorial train and dev data is located in the same folder, so it is enought to add the path of the data directory to our config\n",
        "config.model.dataset.data_dir = DATA_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB96-3sTc3yk",
        "colab_type": "text"
      },
      "source": [
        "# Building the PyTorch Lightning Trainer\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
        "\n",
        "Lets first instantiate a Trainer object!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tG4FzZ4Ui60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knF6QeQQdMrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets modify some trainer configs\n",
        "# checks if we have GPU available and uses it\n",
        "cuda = 1 if torch.cuda.is_available() else 0\n",
        "config.trainer.gpus = cuda\n",
        "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
        "\n",
        "# For mixed precision training, use precision=16 and amp_level=O1\n",
        "\n",
        "# Reduces maximum number of epochs to 1 for a quick training\n",
        "config.trainer.max_epochs = 1\n",
        "\n",
        "# Remove distributed training flags\n",
        "config.trainer.distributed_backend = None\n",
        "\n",
        "trainer = pl.Trainer(**config.trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IlEMdVxdr6p",
        "colab_type": "text"
      },
      "source": [
        "# Setting up a NeMo Experiment¶\n",
        "\n",
        "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uztqGAmdrYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
        "\n",
        "# the exp_dir provides a path to the current experiment for easy access\n",
        "exp_dir = str(exp_dir)\n",
        "exp_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FI_nQsJo_11",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tjLhUvL_o7_",
        "colab_type": "text"
      },
      "source": [
        "Before initializing the model, we might want to modify some of the model configs. For example, we might want to modify the pretrained BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeuc2i7Y_nP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# complete list of supported BERT-like models\n",
        "nemo_nlp.modules.get_pretrained_lm_models_list()\n",
        "\n",
        "# specify BERT-like model, you want to use\n",
        "PRETRAINED_BERT_MODEL = \"bert-base-uncased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK2xglXyAUOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the specified above model parameters to the config\n",
        "config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzNZNAVRjDD-",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation.\n",
        "Also, the pretrained BERT model will be downloaded, note it can take up to a few minutes depending on the size of the chosen BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgsGLydWo-6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.trainer.max_epochs = 10\n",
        "model = nemo_nlp.models.TokenClassificationModel(cfg=config.model, trainer=trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ592Tx4pzyB",
        "colab_type": "text"
      },
      "source": [
        "## Monitoring training progress\n",
        "Optionally, you can create a Tensorboard visualization to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTJr16_pp0aS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {exp_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUvnSpyjp0Dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start model training\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxBiIKMlH8yv",
        "colab_type": "text"
      },
      "source": [
        "After training for 5 epochs, with the default config, your model performance should look similar to this: \n",
        "```\n",
        "    label                                                precision    recall       f1           support   \n",
        "    O (label_id: 0)                                         99.14      99.19      99.17     131141\n",
        "    B-GPE (label_id: 1)                                     95.86      94.03      94.93       2362\n",
        "    B-LOC (label_id: 2)                                     83.99      90.31      87.04       5346\n",
        "    B-MISC (label_id: 3)                                    39.82      34.62      37.04        130\n",
        "    B-ORG (label_id: 4)                                     78.33      67.82      72.70       2980\n",
        "    B-PER (label_id: 5)                                     84.36      84.32      84.34       2577\n",
        "    B-TIME (label_id: 6)                                    91.94      91.23      91.58       2975\n",
        "    I-GPE (label_id: 7)                                     88.89      34.78      50.00         23\n",
        "    I-LOC (label_id: 8)                                     77.18      79.13      78.14       1030\n",
        "    I-MISC (label_id: 9)                                    28.57      24.00      26.09         75\n",
        "    I-ORG (label_id: 10)                                    78.67      75.67      77.14       2384\n",
        "    I-PER (label_id: 11)                                    86.69      90.17      88.40       2687\n",
        "    I-TIME (label_id: 12)                                   83.21      83.48      83.34        938\n",
        "    -------------------\n",
        "    micro avg                                               96.95      96.95      96.95     154648\n",
        "    macro avg                                               78.20      72.98      74.61     154648\n",
        "    weighted avg                                            96.92      96.95      96.92     154648\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPdzJVAgSFaJ",
        "colab_type": "text"
      },
      "source": [
        "# Inference\n",
        "\n",
        "To see how the model performs, let’s run inference on a few examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQhsamclRtxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the list of queiries for inference\n",
        "queries = [\n",
        "    'we bought four shirts from the nvidia gear store in santa clara.',\n",
        "    'Nvidia is a company.',\n",
        "    'The Adventures of Tom Sawyer by Mark Twain is an 1876 novel about a young boy growing '\n",
        "    + 'up along the Mississippi River.',\n",
        "]\n",
        "results = model.add_enties(queries)\n",
        "\n",
        "for query, result in zip(queries, results):\n",
        "    logging.info('')\n",
        "    logging.info(f'Query : {query}')\n",
        "    logging.info(f'Result: {result.strip()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaW0A1OOwefR",
        "colab_type": "text"
      },
      "source": [
        "We can also use our model to generate predictions for a dataset from a file, for example, for final evaluation or to perform error analysis. Below we are using the same dev set we were using for evaluation for training, but it could be any text file as long as it follows the data format described above.\n",
        "Labels_file is optional here, and if provided will be used to get metrics and plot confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglcZV1bwsv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run evaluation on a dataset from file\n",
        "model.evaluate_from_file(\n",
        "    text_file=os.path.join(cfg.model.dataset.data_dir, cfg.model.validation_ds.text_file),\n",
        "    labels_file=os.path.join(cfg.model.dataset.data_dir, cfg.model.validation_ds.labels_file),\n",
        "    output_dir=exp_dir,\n",
        "    add_confusion_matrix=True,\n",
        "    normalize_confusion_matrix=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ref1qSonGNhP",
        "colab_type": "text"
      },
      "source": [
        "# Training Script\n",
        "\n",
        "If you have NeMo installed locally, you can also train the model with `nlp/token_classification/token_classification.py.`\n",
        "\n",
        "To run training script, use:\n",
        "\n",
        "`python token_classification.py model.dataset.data_dir=PATH_TO_DATA_DIR`"
      ]
    }
  ]
}